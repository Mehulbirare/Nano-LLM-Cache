# ğŸš€ Nano-LLM-Cache - Quick Start Guide

## Installation

```bash
cd c:\Users\mehul\Projects\nano-llm-cache
npm install
```

## Build the Library

```bash
npm run build
```

This will create the distribution files in the `dist/` folder:
- `dist/index.js` - CommonJS build
- `dist/index.mjs` - ES Module build
- `dist/index.d.ts` - TypeScript definitions

## Run the Demo

```bash
node demo-node.mjs
```

This will demonstrate:
1. Vector similarity calculations
2. Saving prompts to cache
3. Querying with exact matches
4. Querying with semantically similar prompts
5. Cache statistics

## Project Structure

```
nano-llm-cache/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ index.ts           # Main entry point
â”‚   â”œâ”€â”€ types.ts           # TypeScript type definitions
â”‚   â”œâ”€â”€ cache.ts           # Core NanoCache class
â”‚   â”œâ”€â”€ embeddings.ts      # Embedding generator using @xenova/transformers
â”‚   â”œâ”€â”€ storage.ts         # IndexedDB storage layer
â”‚   â”œâ”€â”€ similarity.ts      # Vector similarity calculations
â”‚   â””â”€â”€ __tests__/         # Test files
â”‚       â”œâ”€â”€ cache.test.ts
â”‚       â””â”€â”€ similarity.test.ts
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ basic-usage.ts     # Comprehensive examples
â”‚   â””â”€â”€ browser-demo.html  # Interactive browser demo
â”œâ”€â”€ dist/                  # Built files (generated)
â”œâ”€â”€ package.json
â”œâ”€â”€ tsconfig.json
â”œâ”€â”€ vitest.config.ts
â”œâ”€â”€ README.md              # Full documentation
â”œâ”€â”€ CHANGELOG.md
â”œâ”€â”€ CONTRIBUTING.md
â””â”€â”€ LICENSE

```

## Usage Examples

### Basic Usage

```typescript
import { NanoCache } from 'nano-llm-cache';

const cache = new NanoCache({
  similarityThreshold: 0.95,
  maxAge: 60 * 60 * 1000, // 1 hour
  debug: true
});

// Save a response
await cache.save(
  'What is the weather in London?',
  'Cloudy, 15Â°C'
);

// Query with similar prompt
const result = await cache.query('Tell me the London weather');

if (result.hit) {
  console.log('Cache hit!', result.response);
}
```

### OpenAI Wrapper

```typescript
import OpenAI from 'openai';
import { NanoCache } from 'nano-llm-cache';

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
const cache = new NanoCache({ similarityThreshold: 0.95 });

// Wrap the OpenAI function
const cachedCreate = cache.createChatWrapper(
  openai.chat.completions.create.bind(openai.chat.completions)
);

// Use it like normal OpenAI - but with caching!
const response = await cachedCreate({
  model: 'gpt-4',
  messages: [{ role: 'user', content: 'How do I center a div?' }]
});
```

## Key Features

âœ… **Semantic Caching** - Matches by meaning, not exact text
âœ… **Local Embeddings** - Privacy-first, runs entirely client-side
âœ… **Persistent Storage** - IndexedDB for cross-session caching
âœ… **TTL Support** - Configurable expiration times
âœ… **OpenAI Compatible** - Drop-in wrapper for OpenAI SDK
âœ… **TypeScript** - Full type safety and IntelliSense

## Configuration Options

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `similarityThreshold` | number | 0.95 | Min similarity (0-1) for cache hit |
| `maxAge` | number | undefined | Max age in ms before expiration |
| `modelName` | string | 'Xenova/all-MiniLM-L6-v2' | Embedding model |
| `debug` | boolean | false | Enable debug logging |
| `storagePrefix` | string | 'nano-llm-cache' | Storage key prefix |

## Testing

Run tests:
```bash
npm test
```

Run tests with UI:
```bash
npm run test:ui
```

## Development

Watch mode for development:
```bash
npm run dev
```

## Publishing to NPM

1. Update version in `package.json`
2. Build: `npm run build`
3. Publish: `npm publish`

## Browser Demo

Open `examples/browser-demo.html` in a browser to see an interactive demo with a beautiful UI.

## Next Steps

1. Read the full [README.md](README.md) for detailed documentation
2. Check out [examples/basic-usage.ts](examples/basic-usage.ts) for more examples
3. Review [CONTRIBUTING.md](CONTRIBUTING.md) if you want to contribute

## Support

- GitHub Issues: [Report bugs or request features]
- Documentation: See README.md
- Examples: See examples/ folder

---

**Made with â¤ï¸ for developers who want to save money on LLM API calls!**
